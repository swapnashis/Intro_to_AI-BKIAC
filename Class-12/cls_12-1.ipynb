{"cells":[{"cell_type":"markdown","metadata":{"id":"GPcG0rC6uTAY"},"source":["## Model run on MNIST"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":59892,"status":"ok","timestamp":1721038640004,"user":{"displayName":"Swapnashis Bhattacharjee","userId":"07614146342855752455"},"user_tz":-360},"id":"p_v209oucxnT","outputId":"445e5ac8-0327-4876-bf76-ae1ef167e4ff"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../../data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 9912422/9912422 [00:00\u003c00:00, 17379849.60it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ../../data/MNIST/raw/train-images-idx3-ubyte.gz to ../../data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 28881/28881 [00:00\u003c00:00, 474333.23it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ../../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../../data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1648877/1648877 [00:00\u003c00:00, 4428276.23it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ../../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../../data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4542/4542 [00:00\u003c00:00, 2797023.75it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ../../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../../data/MNIST/raw\n","\n","Epoch [1/6], Step [100/600], Loss: 0.2542\n","Epoch [1/6], Step [200/600], Loss: 0.0564\n","Epoch [1/6], Step [300/600], Loss: 0.1019\n","Epoch [1/6], Step [400/600], Loss: 0.0379\n","Epoch [1/6], Step [500/600], Loss: 0.1652\n","Epoch [1/6], Step [600/600], Loss: 0.0317\n","Epoch [2/6], Step [100/600], Loss: 0.0275\n","Epoch [2/6], Step [200/600], Loss: 0.0188\n","Epoch [2/6], Step [300/600], Loss: 0.0345\n","Epoch [2/6], Step [400/600], Loss: 0.0487\n","Epoch [2/6], Step [500/600], Loss: 0.0489\n","Epoch [2/6], Step [600/600], Loss: 0.0621\n","Epoch [3/6], Step [100/600], Loss: 0.0251\n","Epoch [3/6], Step [200/600], Loss: 0.0122\n","Epoch [3/6], Step [300/600], Loss: 0.0897\n","Epoch [3/6], Step [400/600], Loss: 0.0160\n","Epoch [3/6], Step [500/600], Loss: 0.0437\n","Epoch [3/6], Step [600/600], Loss: 0.0472\n","Epoch [4/6], Step [100/600], Loss: 0.0476\n","Epoch [4/6], Step [200/600], Loss: 0.1255\n","Epoch [4/6], Step [300/600], Loss: 0.0229\n","Epoch [4/6], Step [400/600], Loss: 0.0044\n","Epoch [4/6], Step [500/600], Loss: 0.0187\n","Epoch [4/6], Step [600/600], Loss: 0.0073\n","Epoch [5/6], Step [100/600], Loss: 0.0306\n","Epoch [5/6], Step [200/600], Loss: 0.0265\n","Epoch [5/6], Step [300/600], Loss: 0.0292\n","Epoch [5/6], Step [400/600], Loss: 0.0046\n","Epoch [5/6], Step [500/600], Loss: 0.0463\n","Epoch [5/6], Step [600/600], Loss: 0.0043\n","Epoch [6/6], Step [100/600], Loss: 0.0255\n","Epoch [6/6], Step [200/600], Loss: 0.0265\n","Epoch [6/6], Step [300/600], Loss: 0.0284\n","Epoch [6/6], Step [400/600], Loss: 0.0062\n","Epoch [6/6], Step [500/600], Loss: 0.0051\n","Epoch [6/6], Step [600/600], Loss: 0.0047\n","Test Accuracy of the model on the 10000 test images: 99.02 %\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","\n","# Device configuration\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","# Hyper parameters\n","num_epochs = 6\n","num_classes = 10\n","batch_size = 100\n","learning_rate = 0.001\n","\n","# MNIST dataset\n","train_dataset = torchvision.datasets.MNIST(root='../../data/',\n","                                           train=True,\n","                                           transform=transforms.ToTensor(),\n","                                           download=True)\n","\n","test_dataset = torchvision.datasets.MNIST(root='../../data/',\n","                                          train=False,\n","                                          transform=transforms.ToTensor())\n","\n","# Data loader\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=batch_size,\n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                          batch_size=batch_size,\n","                                          shuffle=False)\n","\n","# Convolutional neural network (two convolutional layers)\n","class ConvNet(nn.Module):\n","    def __init__(self, num_classes=10):\n","        super(ConvNet, self).__init__()\n","        self.layer1 = nn.Sequential(\n","            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n","            nn.BatchNorm2d(16),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2))\n","        self.layer2 = nn.Sequential(\n","            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2))\n","        self.fc = nn.Linear(7*7*32, num_classes)\n","\n","    def forward(self, x):\n","        out = self.layer1(x)\n","        out = self.layer2(out)\n","        out = out.reshape(out.size(0), -1)\n","        out = self.fc(out)\n","        return out\n","\n","model = ConvNet(num_classes).to(device)\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Train the model\n","total_step = len(train_loader)\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        # Forward pass\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (i+1) % 100 == 0:\n","            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n","                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n","\n","# Test the model\n","model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for images, labels in test_loader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n","\n","# Save the model checkpoint\n","torch.save(model.state_dict(), 'model.ckpt')"]},{"cell_type":"markdown","metadata":{"id":"Gl-1_PmYiE6N"},"source":["\n","## Model run on CFAR10"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23154,"status":"ok","timestamp":1721052264024,"user":{"displayName":"Swapnashis Bhattacharjee","userId":"07614146342855752455"},"user_tz":-360},"id":"r3GZxmYSjeoB","outputId":"cb386c29-de8e-4aa3-ef63-893717e16451"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../../data/cifar-10-python.tar.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 170498071/170498071 [00:10\u003c00:00, 15594271.74it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ../../data/cifar-10-python.tar.gz to ../../data/\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","\n","# Device configuration\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","# Hyper parameters\n","num_epochs = 1000\n","num_classes = 10\n","batch_size = 100\n","learning_rate = 0.001\n","\n","# CIFAR10 dataset\n","train_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n","                                           train=True,\n","                                           transform=transforms.ToTensor(),\n","                                           download=True)\n","\n","test_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n","                                          train=False,\n","                                          transform=transforms.ToTensor())\n","\n","# Data loader\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=batch_size,\n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                          batch_size=batch_size,\n","                                          shuffle=False)\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1721052264027,"user":{"displayName":"Swapnashis Bhattacharjee","userId":"07614146342855752455"},"user_tz":-360},"id":"x-ZTUnvIkPQB"},"outputs":[],"source":["class ConvNet(nn.Module):\n","    def __init__(self, num_classes=10):\n","        super(ConvNet, self).__init__()\n","        self.layer1 = nn.Sequential(\n","            nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=2),\n","            nn.BatchNorm2d(16),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2))\n","        self.layer2 = nn.Sequential(\n","            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2))\n","        self.fc = nn.Linear(8*8*32, num_classes)\n","\n","    def forward(self, x):\n","        out = self.layer1(x)\n","        out = self.layer2(out)\n","        out = out.reshape(out.size(0), -1)\n","        out = self.fc(out)\n","        return out\n","\n","model = ConvNet(num_classes).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"TgZgCrdwkiMo"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/1000], Step [100/500], Loss: 1.4936\n","Epoch [1/1000], Step [200/500], Loss: 1.2779\n","Epoch [1/1000], Step [300/500], Loss: 1.3757\n","Epoch [1/1000], Step [400/500], Loss: 1.3038\n","Epoch [1/1000], Step [500/500], Loss: 0.9991\n","Epoch [2/1000], Step [100/500], Loss: 0.9139\n","Epoch [2/1000], Step [200/500], Loss: 0.9062\n","Epoch [2/1000], Step [300/500], Loss: 1.0716\n","Epoch [2/1000], Step [400/500], Loss: 1.0487\n","Epoch [2/1000], Step [500/500], Loss: 0.8149\n","Epoch [3/1000], Step [100/500], Loss: 0.8414\n","Epoch [3/1000], Step [200/500], Loss: 0.9665\n","Epoch [3/1000], Step [300/500], Loss: 0.8774\n","Epoch [3/1000], Step [400/500], Loss: 1.0898\n","Epoch [3/1000], Step [500/500], Loss: 1.0410\n","Epoch [4/1000], Step [100/500], Loss: 0.7681\n","Epoch [4/1000], Step [200/500], Loss: 0.9455\n","Epoch [4/1000], Step [300/500], Loss: 0.9842\n","Epoch [4/1000], Step [400/500], Loss: 0.9973\n","Epoch [4/1000], Step [500/500], Loss: 0.9768\n","Epoch [5/1000], Step [100/500], Loss: 0.7967\n","Epoch [5/1000], Step [200/500], Loss: 0.8907\n","Epoch [5/1000], Step [300/500], Loss: 0.8730\n","Epoch [5/1000], Step [400/500], Loss: 0.8406\n","Epoch [5/1000], Step [500/500], Loss: 0.8884\n","Epoch [6/1000], Step [100/500], Loss: 0.6254\n","Epoch [6/1000], Step [200/500], Loss: 0.7478\n","Epoch [6/1000], Step [300/500], Loss: 0.6672\n","Epoch [6/1000], Step [400/500], Loss: 0.8913\n","Epoch [6/1000], Step [500/500], Loss: 0.8104\n","Epoch [7/1000], Step [100/500], Loss: 0.7875\n","Epoch [7/1000], Step [200/500], Loss: 0.8176\n","Epoch [7/1000], Step [300/500], Loss: 0.7384\n","Epoch [7/1000], Step [400/500], Loss: 0.7874\n","Epoch [7/1000], Step [500/500], Loss: 0.8003\n","Epoch [8/1000], Step [100/500], Loss: 0.7551\n","Epoch [8/1000], Step [200/500], Loss: 0.6361\n","Epoch [8/1000], Step [300/500], Loss: 0.7539\n","Epoch [8/1000], Step [400/500], Loss: 0.7328\n","Epoch [8/1000], Step [500/500], Loss: 0.5258\n","Epoch [9/1000], Step [100/500], Loss: 0.6167\n","Epoch [9/1000], Step [200/500], Loss: 0.6570\n","Epoch [9/1000], Step [300/500], Loss: 0.6760\n","Epoch [9/1000], Step [400/500], Loss: 0.7880\n","Epoch [9/1000], Step [500/500], Loss: 0.7592\n","Epoch [10/1000], Step [100/500], Loss: 0.6995\n","Epoch [10/1000], Step [200/500], Loss: 0.6569\n","Epoch [10/1000], Step [300/500], Loss: 0.6970\n","Epoch [10/1000], Step [400/500], Loss: 0.7212\n","Epoch [10/1000], Step [500/500], Loss: 0.5911\n","Epoch [11/1000], Step [100/500], Loss: 0.5339\n","Epoch [11/1000], Step [200/500], Loss: 0.6139\n","Epoch [11/1000], Step [300/500], Loss: 0.7340\n","Epoch [11/1000], Step [400/500], Loss: 0.7088\n","Epoch [11/1000], Step [500/500], Loss: 0.6184\n","Epoch [12/1000], Step [100/500], Loss: 0.5637\n","Epoch [12/1000], Step [200/500], Loss: 0.5743\n","Epoch [12/1000], Step [300/500], Loss: 0.7132\n","Epoch [12/1000], Step [400/500], Loss: 0.5331\n","Epoch [12/1000], Step [500/500], Loss: 0.5299\n","Epoch [13/1000], Step [100/500], Loss: 0.5500\n","Epoch [13/1000], Step [200/500], Loss: 0.6149\n","Epoch [13/1000], Step [300/500], Loss: 0.7602\n","Epoch [13/1000], Step [400/500], Loss: 0.5315\n","Epoch [13/1000], Step [500/500], Loss: 0.8773\n","Epoch [14/1000], Step [100/500], Loss: 0.5677\n","Epoch [14/1000], Step [200/500], Loss: 0.6859\n","Epoch [14/1000], Step [300/500], Loss: 0.6685\n","Epoch [14/1000], Step [400/500], Loss: 0.6976\n","Epoch [14/1000], Step [500/500], Loss: 0.5763\n","Epoch [15/1000], Step [100/500], Loss: 0.5577\n","Epoch [15/1000], Step [200/500], Loss: 0.5127\n","Epoch [15/1000], Step [300/500], Loss: 0.4962\n","Epoch [15/1000], Step [400/500], Loss: 0.5325\n","Epoch [15/1000], Step [500/500], Loss: 0.6570\n","Epoch [16/1000], Step [100/500], Loss: 0.4421\n","Epoch [16/1000], Step [200/500], Loss: 0.4848\n","Epoch [16/1000], Step [300/500], Loss: 0.8573\n","Epoch [16/1000], Step [400/500], Loss: 0.5894\n","Epoch [16/1000], Step [500/500], Loss: 0.5284\n","Epoch [17/1000], Step [100/500], Loss: 0.7363\n","Epoch [17/1000], Step [200/500], Loss: 0.5012\n","Epoch [17/1000], Step [300/500], Loss: 0.4683\n","Epoch [17/1000], Step [400/500], Loss: 0.4699\n","Epoch [17/1000], Step [500/500], Loss: 0.7299\n","Epoch [18/1000], Step [100/500], Loss: 0.4539\n","Epoch [18/1000], Step [200/500], Loss: 0.5917\n","Epoch [18/1000], Step [300/500], Loss: 0.6024\n","Epoch [18/1000], Step [400/500], Loss: 0.6884\n","Epoch [18/1000], Step [500/500], Loss: 0.6000\n","Epoch [19/1000], Step [100/500], Loss: 0.6072\n","Epoch [19/1000], Step [200/500], Loss: 0.3592\n","Epoch [19/1000], Step [300/500], Loss: 0.5814\n","Epoch [19/1000], Step [400/500], Loss: 0.6584\n","Epoch [19/1000], Step [500/500], Loss: 0.5075\n","Epoch [20/1000], Step [100/500], Loss: 0.5705\n","Epoch [20/1000], Step [200/500], Loss: 0.5575\n","Epoch [20/1000], Step [300/500], Loss: 0.5858\n","Epoch [20/1000], Step [400/500], Loss: 0.5610\n","Epoch [20/1000], Step [500/500], Loss: 0.6456\n","Epoch [21/1000], Step [100/500], Loss: 0.3729\n","Epoch [21/1000], Step [200/500], Loss: 0.6126\n","Epoch [21/1000], Step [300/500], Loss: 0.6558\n","Epoch [21/1000], Step [400/500], Loss: 0.4378\n","Epoch [21/1000], Step [500/500], Loss: 0.5440\n","Epoch [22/1000], Step [100/500], Loss: 0.4122\n","Epoch [22/1000], Step [200/500], Loss: 0.3652\n","Epoch [22/1000], Step [300/500], Loss: 0.4971\n","Epoch [22/1000], Step [400/500], Loss: 0.4965\n","Epoch [22/1000], Step [500/500], Loss: 0.4070\n","Epoch [23/1000], Step [100/500], Loss: 0.4143\n","Epoch [23/1000], Step [200/500], Loss: 0.4792\n","Epoch [23/1000], Step [300/500], Loss: 0.4332\n","Epoch [23/1000], Step [400/500], Loss: 0.4255\n","Epoch [23/1000], Step [500/500], Loss: 0.5973\n","Epoch [24/1000], Step [100/500], Loss: 0.3194\n","Epoch [24/1000], Step [200/500], Loss: 0.4638\n","Epoch [24/1000], Step [300/500], Loss: 0.4984\n","Epoch [24/1000], Step [400/500], Loss: 0.4817\n","Epoch [24/1000], Step [500/500], Loss: 0.3744\n","Epoch [25/1000], Step [100/500], Loss: 0.5679\n","Epoch [25/1000], Step [200/500], Loss: 0.4585\n","Epoch [25/1000], Step [300/500], Loss: 0.4954\n","Epoch [25/1000], Step [400/500], Loss: 0.6265\n","Epoch [25/1000], Step [500/500], Loss: 0.5174\n","Epoch [26/1000], Step [100/500], Loss: 0.4523\n","Epoch [26/1000], Step [200/500], Loss: 0.3883\n","Epoch [26/1000], Step [300/500], Loss: 0.4749\n","Epoch [26/1000], Step [400/500], Loss: 0.4139\n","Epoch [26/1000], Step [500/500], Loss: 0.2924\n","Epoch [27/1000], Step [100/500], Loss: 0.3671\n","Epoch [27/1000], Step [200/500], Loss: 0.6770\n","Epoch [27/1000], Step [300/500], Loss: 0.3419\n","Epoch [27/1000], Step [400/500], Loss: 0.6546\n","Epoch [27/1000], Step [500/500], Loss: 0.3476\n","Epoch [28/1000], Step [100/500], Loss: 0.2902\n","Epoch [28/1000], Step [200/500], Loss: 0.3755\n","Epoch [28/1000], Step [300/500], Loss: 0.3969\n","Epoch [28/1000], Step [400/500], Loss: 0.3682\n","Epoch [28/1000], Step [500/500], Loss: 0.5468\n","Epoch [29/1000], Step [100/500], Loss: 0.2379\n","Epoch [29/1000], Step [200/500], Loss: 0.5770\n","Epoch [29/1000], Step [300/500], Loss: 0.4449\n","Epoch [29/1000], Step [400/500], Loss: 0.4393\n","Epoch [29/1000], Step [500/500], Loss: 0.4455\n","Epoch [30/1000], Step [100/500], Loss: 0.5479\n","Epoch [30/1000], Step [200/500], Loss: 0.5273\n","Epoch [30/1000], Step [300/500], Loss: 0.3992\n","Epoch [30/1000], Step [400/500], Loss: 0.3782\n","Epoch [30/1000], Step [500/500], Loss: 0.5389\n","Epoch [31/1000], Step [100/500], Loss: 0.4789\n","Epoch [31/1000], Step [200/500], Loss: 0.2665\n","Epoch [31/1000], Step [300/500], Loss: 0.4118\n","Epoch [31/1000], Step [400/500], Loss: 0.5101\n","Epoch [31/1000], Step [500/500], Loss: 0.3838\n","Epoch [32/1000], Step [100/500], Loss: 0.5056\n","Epoch [32/1000], Step [200/500], Loss: 0.4665\n","Epoch [32/1000], Step [300/500], Loss: 0.3648\n","Epoch [32/1000], Step [400/500], Loss: 0.3153\n","Epoch [32/1000], Step [500/500], Loss: 0.2691\n","Epoch [33/1000], Step [100/500], Loss: 0.4507\n","Epoch [33/1000], Step [200/500], Loss: 0.3798\n","Epoch [33/1000], Step [300/500], Loss: 0.3564\n","Epoch [33/1000], Step [400/500], Loss: 0.4183\n","Epoch [33/1000], Step [500/500], Loss: 0.4139\n","Epoch [34/1000], Step [100/500], Loss: 0.3141\n","Epoch [34/1000], Step [200/500], Loss: 0.4150\n","Epoch [34/1000], Step [300/500], Loss: 0.4742\n","Epoch [34/1000], Step [400/500], Loss: 0.3749\n","Epoch [34/1000], Step [500/500], Loss: 0.3850\n","Epoch [35/1000], Step [100/500], Loss: 0.3621\n","Epoch [35/1000], Step [200/500], Loss: 0.2687\n","Epoch [35/1000], Step [300/500], Loss: 0.3904\n","Epoch [35/1000], Step [400/500], Loss: 0.3388\n","Epoch [35/1000], Step [500/500], Loss: 0.5341\n","Epoch [36/1000], Step [100/500], Loss: 0.2881\n","Epoch [36/1000], Step [200/500], Loss: 0.4754\n","Epoch [36/1000], Step [300/500], Loss: 0.5186\n","Epoch [36/1000], Step [400/500], Loss: 0.4570\n","Epoch [36/1000], Step [500/500], Loss: 0.4246\n","Epoch [37/1000], Step [100/500], Loss: 0.2894\n","Epoch [37/1000], Step [200/500], Loss: 0.3258\n","Epoch [37/1000], Step [300/500], Loss: 0.4310\n","Epoch [37/1000], Step [400/500], Loss: 0.3791\n","Epoch [37/1000], Step [500/500], Loss: 0.3777\n","Epoch [38/1000], Step [100/500], Loss: 0.3535\n","Epoch [38/1000], Step [200/500], Loss: 0.2963\n","Epoch [38/1000], Step [300/500], Loss: 0.3089\n","Epoch [38/1000], Step [400/500], Loss: 0.3101\n","Epoch [38/1000], Step [500/500], Loss: 0.3958\n","Epoch [39/1000], Step [100/500], Loss: 0.5324\n","Epoch [39/1000], Step [200/500], Loss: 0.3186\n","Epoch [39/1000], Step [300/500], Loss: 0.2350\n","Epoch [39/1000], Step [400/500], Loss: 0.4060\n","Epoch [39/1000], Step [500/500], Loss: 0.3181\n","Epoch [40/1000], Step [100/500], Loss: 0.2646\n","Epoch [40/1000], Step [200/500], Loss: 0.4103\n","Epoch [40/1000], Step [300/500], Loss: 0.3196\n","Epoch [40/1000], Step [400/500], Loss: 0.3372\n","Epoch [40/1000], Step [500/500], Loss: 0.3179\n","Epoch [41/1000], Step [100/500], Loss: 0.3568\n","Epoch [41/1000], Step [200/500], Loss: 0.4352\n","Epoch [41/1000], Step [300/500], Loss: 0.3770\n","Epoch [41/1000], Step [400/500], Loss: 0.4026\n","Epoch [41/1000], Step [500/500], Loss: 0.4389\n","Epoch [42/1000], Step [100/500], Loss: 0.4085\n","Epoch [42/1000], Step [200/500], Loss: 0.3214\n","Epoch [42/1000], Step [300/500], Loss: 0.3280\n","Epoch [42/1000], Step [400/500], Loss: 0.3455\n","Epoch [42/1000], Step [500/500], Loss: 0.3716\n","Epoch [43/1000], Step [100/500], Loss: 0.2424\n","Epoch [43/1000], Step [200/500], Loss: 0.3708\n","Epoch [43/1000], Step [300/500], Loss: 0.2894\n","Epoch [43/1000], Step [400/500], Loss: 0.4060\n","Epoch [43/1000], Step [500/500], Loss: 0.3253\n","Epoch [44/1000], Step [100/500], Loss: 0.3333\n","Epoch [44/1000], Step [200/500], Loss: 0.3839\n","Epoch [44/1000], Step [300/500], Loss: 0.4637\n","Epoch [44/1000], Step [400/500], Loss: 0.4671\n","Epoch [44/1000], Step [500/500], Loss: 0.5317\n","Epoch [45/1000], Step [100/500], Loss: 0.3786\n","Epoch [45/1000], Step [200/500], Loss: 0.4322\n","Epoch [45/1000], Step [300/500], Loss: 0.2759\n","Epoch [45/1000], Step [400/500], Loss: 0.3337\n","Epoch [45/1000], Step [500/500], Loss: 0.3315\n","Epoch [46/1000], Step [100/500], Loss: 0.2863\n","Epoch [46/1000], Step [200/500], Loss: 0.3927\n","Epoch [46/1000], Step [300/500], Loss: 0.2742\n","Epoch [46/1000], Step [400/500], Loss: 0.3168\n","Epoch [46/1000], Step [500/500], Loss: 0.4252\n","Epoch [47/1000], Step [100/500], Loss: 0.2427\n","Epoch [47/1000], Step [200/500], Loss: 0.4099\n","Epoch [47/1000], Step [300/500], Loss: 0.2972\n","Epoch [47/1000], Step [400/500], Loss: 0.3970\n","Epoch [47/1000], Step [500/500], Loss: 0.3595\n","Epoch [48/1000], Step [100/500], Loss: 0.2611\n","Epoch [48/1000], Step [200/500], Loss: 0.4234\n","Epoch [48/1000], Step [300/500], Loss: 0.3950\n","Epoch [48/1000], Step [400/500], Loss: 0.4563\n","Epoch [48/1000], Step [500/500], Loss: 0.2697\n","Epoch [49/1000], Step [100/500], Loss: 0.2649\n","Epoch [49/1000], Step [200/500], Loss: 0.3019\n","Epoch [49/1000], Step [300/500], Loss: 0.3646\n","Epoch [49/1000], Step [400/500], Loss: 0.4024\n","Epoch [49/1000], Step [500/500], Loss: 0.3212\n","Epoch [50/1000], Step [100/500], Loss: 0.3002\n","Epoch [50/1000], Step [200/500], Loss: 0.2551\n","Epoch [50/1000], Step [300/500], Loss: 0.2155\n","Epoch [50/1000], Step [400/500], Loss: 0.2024\n","Epoch [50/1000], Step [500/500], Loss: 0.1991\n","Epoch [51/1000], Step [100/500], Loss: 0.3151\n","Epoch [51/1000], Step [200/500], Loss: 0.3227\n","Epoch [51/1000], Step [300/500], Loss: 0.3368\n","Epoch [51/1000], Step [400/500], Loss: 0.3524\n","Epoch [51/1000], Step [500/500], Loss: 0.2288\n","Epoch [52/1000], Step [100/500], Loss: 0.1931\n","Epoch [52/1000], Step [200/500], Loss: 0.2134\n","Epoch [52/1000], Step [300/500], Loss: 0.2809\n","Epoch [52/1000], Step [400/500], Loss: 0.2857\n","Epoch [52/1000], Step [500/500], Loss: 0.3767\n","Epoch [53/1000], Step [100/500], Loss: 0.2120\n","Epoch [53/1000], Step [200/500], Loss: 0.3284\n","Epoch [53/1000], Step [300/500], Loss: 0.2452\n","Epoch [53/1000], Step [400/500], Loss: 0.2699\n","Epoch [53/1000], Step [500/500], Loss: 0.2238\n","Epoch [54/1000], Step [100/500], Loss: 0.2623\n","Epoch [54/1000], Step [200/500], Loss: 0.3947\n","Epoch [54/1000], Step [300/500], Loss: 0.2111\n","Epoch [54/1000], Step [400/500], Loss: 0.3087\n","Epoch [54/1000], Step [500/500], Loss: 0.1805\n","Epoch [55/1000], Step [100/500], Loss: 0.1511\n","Epoch [55/1000], Step [200/500], Loss: 0.3823\n","Epoch [55/1000], Step [300/500], Loss: 0.2480\n","Epoch [55/1000], Step [400/500], Loss: 0.3213\n","Epoch [55/1000], Step [500/500], Loss: 0.2458\n","Epoch [56/1000], Step [100/500], Loss: 0.3238\n","Epoch [56/1000], Step [200/500], Loss: 0.2369\n","Epoch [56/1000], Step [300/500], Loss: 0.3009\n","Epoch [56/1000], Step [400/500], Loss: 0.2970\n","Epoch [56/1000], Step [500/500], Loss: 0.4040\n","Epoch [57/1000], Step [100/500], Loss: 0.2542\n","Epoch [57/1000], Step [200/500], Loss: 0.3727\n","Epoch [57/1000], Step [300/500], Loss: 0.3164\n","Epoch [57/1000], Step [400/500], Loss: 0.2400\n","Epoch [57/1000], Step [500/500], Loss: 0.4244\n","Epoch [58/1000], Step [100/500], Loss: 0.2275\n","Epoch [58/1000], Step [200/500], Loss: 0.2593\n","Epoch [58/1000], Step [300/500], Loss: 0.2835\n","Epoch [58/1000], Step [400/500], Loss: 0.3170\n","Epoch [58/1000], Step [500/500], Loss: 0.3160\n","Epoch [59/1000], Step [100/500], Loss: 0.2328\n","Epoch [59/1000], Step [200/500], Loss: 0.1840\n","Epoch [59/1000], Step [300/500], Loss: 0.1844\n","Epoch [59/1000], Step [400/500], Loss: 0.2753\n","Epoch [59/1000], Step [500/500], Loss: 0.2860\n","Epoch [60/1000], Step [100/500], Loss: 0.1939\n","Epoch [60/1000], Step [200/500], Loss: 0.3329\n","Epoch [60/1000], Step [300/500], Loss: 0.2050\n","Epoch [60/1000], Step [400/500], Loss: 0.2300\n","Epoch [60/1000], Step [500/500], Loss: 0.3044\n","Epoch [61/1000], Step [100/500], Loss: 0.2835\n","Epoch [61/1000], Step [200/500], Loss: 0.2360\n","Epoch [61/1000], Step [300/500], Loss: 0.2897\n","Epoch [61/1000], Step [400/500], Loss: 0.2527\n","Epoch [61/1000], Step [500/500], Loss: 0.2016\n","Epoch [62/1000], Step [100/500], Loss: 0.1738\n","Epoch [62/1000], Step [200/500], Loss: 0.3271\n","Epoch [62/1000], Step [300/500], Loss: 0.2810\n","Epoch [62/1000], Step [400/500], Loss: 0.3735\n","Epoch [62/1000], Step [500/500], Loss: 0.2068\n","Epoch [63/1000], Step [100/500], Loss: 0.3059\n","Epoch [63/1000], Step [200/500], Loss: 0.1850\n","Epoch [63/1000], Step [300/500], Loss: 0.1736\n","Epoch [63/1000], Step [400/500], Loss: 0.1958\n","Epoch [63/1000], Step [500/500], Loss: 0.2736\n","Epoch [64/1000], Step [100/500], Loss: 0.1714\n","Epoch [64/1000], Step [200/500], Loss: 0.1440\n","Epoch [64/1000], Step [300/500], Loss: 0.1868\n","Epoch [64/1000], Step [400/500], Loss: 0.2308\n","Epoch [64/1000], Step [500/500], Loss: 0.2797\n","Epoch [65/1000], Step [100/500], Loss: 0.2151\n","Epoch [65/1000], Step [200/500], Loss: 0.1952\n","Epoch [65/1000], Step [300/500], Loss: 0.3034\n","Epoch [65/1000], Step [400/500], Loss: 0.3303\n","Epoch [65/1000], Step [500/500], Loss: 0.2346\n","Epoch [66/1000], Step [100/500], Loss: 0.3155\n","Epoch [66/1000], Step [200/500], Loss: 0.2682\n","Epoch [66/1000], Step [300/500], Loss: 0.1494\n","Epoch [66/1000], Step [400/500], Loss: 0.2948\n","Epoch [66/1000], Step [500/500], Loss: 0.4312\n","Epoch [67/1000], Step [100/500], Loss: 0.2197\n","Epoch [67/1000], Step [200/500], Loss: 0.2572\n","Epoch [67/1000], Step [300/500], Loss: 0.2157\n","Epoch [67/1000], Step [400/500], Loss: 0.4154\n","Epoch [67/1000], Step [500/500], Loss: 0.1600\n","Epoch [68/1000], Step [100/500], Loss: 0.2373\n","Epoch [68/1000], Step [200/500], Loss: 0.2528\n","Epoch [68/1000], Step [300/500], Loss: 0.4197\n","Epoch [68/1000], Step [400/500], Loss: 0.2946\n","Epoch [68/1000], Step [500/500], Loss: 0.2701\n","Epoch [69/1000], Step [100/500], Loss: 0.1901\n","Epoch [69/1000], Step [200/500], Loss: 0.2858\n","Epoch [69/1000], Step [300/500], Loss: 0.2927\n","Epoch [69/1000], Step [400/500], Loss: 0.3627\n","Epoch [69/1000], Step [500/500], Loss: 0.2253\n","Epoch [70/1000], Step [100/500], Loss: 0.3593\n","Epoch [70/1000], Step [200/500], Loss: 0.1586\n","Epoch [70/1000], Step [300/500], Loss: 0.1975\n","Epoch [70/1000], Step [400/500], Loss: 0.2745\n","Epoch [70/1000], Step [500/500], Loss: 0.3187\n","Epoch [71/1000], Step [100/500], Loss: 0.3878\n","Epoch [71/1000], Step [200/500], Loss: 0.1193\n","Epoch [71/1000], Step [300/500], Loss: 0.1879\n","Epoch [71/1000], Step [400/500], Loss: 0.2345\n","Epoch [71/1000], Step [500/500], Loss: 0.3495\n","Epoch [72/1000], Step [100/500], Loss: 0.2205\n","Epoch [72/1000], Step [200/500], Loss: 0.1952\n","Epoch [72/1000], Step [300/500], Loss: 0.1766\n","Epoch [72/1000], Step [400/500], Loss: 0.2438\n","Epoch [72/1000], Step [500/500], Loss: 0.2905\n","Epoch [73/1000], Step [100/500], Loss: 0.1950\n","Epoch [73/1000], Step [200/500], Loss: 0.3536\n","Epoch [73/1000], Step [300/500], Loss: 0.3358\n","Epoch [73/1000], Step [400/500], Loss: 0.2055\n","Epoch [73/1000], Step [500/500], Loss: 0.3485\n","Epoch [74/1000], Step [100/500], Loss: 0.1744\n","Epoch [74/1000], Step [200/500], Loss: 0.3131\n","Epoch [74/1000], Step [300/500], Loss: 0.2349\n","Epoch [74/1000], Step [400/500], Loss: 0.3182\n","Epoch [74/1000], Step [500/500], Loss: 0.2477\n","Epoch [75/1000], Step [100/500], Loss: 0.1857\n","Epoch [75/1000], Step [200/500], Loss: 0.2265\n","Epoch [75/1000], Step [300/500], Loss: 0.3509\n"]}],"source":["# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Train the model\n","total_step = len(train_loader)\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        # Forward pass\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (i+1) % 100 == 0:\n","            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n","                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n","\n","# Test the model\n","model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for images, labels in test_loader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n","\n","# Save the model checkpoint\n","torch.save(model.state_dict(), 'model.ckpt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EDfB3lpfo7Rq"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","plt.imshow(test_dataset[8][0][0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-RAB4szvp4yh"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNIA5OgKlLJvQl1vxg5z2a3","gpuType":"T4","name":"","provenance":[{"file_id":"1VcoyHNs8FdOlC1w7-tTPAUbcYYEDvPU5","timestamp":1720964911713}],"toc_visible":true,"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}